# 融合方式: attention+stacking (xgboost)

**Test Accuracy:** 0.9802

**Macro F1:** 0.9796

**分类报告:**

              precision    recall  f1-score   support

           0     1.0000    0.9994    0.9997      1648
           1     0.9957    0.9935    0.9946      1393
           2     0.9841    0.9876    0.9859      1130
           3     0.9952    0.9856    0.9904       209
           4     0.9322    0.9285    0.9304      1511
           5     0.9865    0.9765    0.9815      1194
           6     0.9995    0.9970    0.9982      1968
           7     1.0000    0.9988    0.9994      1657
           8     0.9071    0.9305    0.9186      1165
           9     0.9982    0.9955    0.9968      1107

    accuracy                         0.9802     12982
   macro avg     0.9798    0.9793    0.9796     12982
weighted avg     0.9804    0.9802    0.9803     12982


**混淆矩阵:**

[[1647    0    1    0    0    0    0    0    0    0]
 [   0 1384    6    1    1    0    1    0    0    0]
 [   0    2 1116    0    8    2    0    0    2    0]
 [   0    0    0  206    3    0    0    0    0    0]
 [   0    3    5    0 1403    8    0    0   91    1]
 [   0    0    3    0   14 1166    0    0   11    0]
 [   0    0    2    0    2    0 1962    0    2    0]
 [   0    0    0    0    0    0    0 1655    1    1]
 [   0    1    1    0   73    6    0    0 1084    0]
 [   0    0    0    0    1    0    0    0    4 1102]]

![Confusion Matrix](confusion_matrix_attention_stacking_xgboost_20260215_222413.png)
![Metrics Curve](metrics_curve_attention_stacking_20260215_222413.png)
